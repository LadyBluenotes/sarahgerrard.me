---
title: Introduction to Big O Notation
summary: An generalized introduction to Big O Notation, in the context of software development.
draft: true
date: 2025-11-22
---

Understanding how efficient your code is, is crucial to writing high-quality software. As data scales, it becomes more
important to understand how the performance of your code scales as well. This is where the concept of **Big O Notation**
comes in.

At its core, Big O notation is a mathematical expression that describes the performance of an algorithm or data
structure as a function of its input size. It primarily focuses on the *worst-case scenario*, providing a high-level
understanding of how operations will scale.

## How Big O Notation Works

Big O notation quantifies the efficiency of your code. This efficiency is measured in two ways: **time complexity** and
**space complexity**. Time complexity describes how the execution time scales as the input size increases, while space
complexity how memory usage scales.

It's important to remember that Big O notation is an *expression* describing performance, not an exact number. It is
written as `O(f(n))`, where `f(n)` is the function describing the performance of your algorithm, and `n` is the size of
the input.

## Time Complexity

## Space Complexity

